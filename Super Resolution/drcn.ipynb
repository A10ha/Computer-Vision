{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting torchinfo\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/86/3f/1ca9c336e489b2b7ae3f8cc8a361940ce67ffafc184bda0c3e0dc761eaae/torchinfo-1.5.2-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tqdm\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7a/ec/f8ff3ccfc4e59ce619a66a0bf29dc3b49c2e8c07de29d572e191c006eaa2/tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 639 kB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/sd/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed tqdm-4.61.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Creating Dataset ]\n",
      "Crop Size : [128, 128]\n",
      "Target       : dataset/train\n",
      "Dataset       : /home/sd/zsq_python/DIV2K_train_HR/\n",
      "Format    : PNG\n",
      "Max N    : 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "itr: 799/800: 100%|██████████| 800/800 [12:04<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "import random\n",
    "\n",
    "def _compress(tensor, bit):\n",
    "    max_val = 2**bit - 1\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0) * max_val\n",
    "    tensor = torch.round(tensor)\n",
    "    tensor = tensor / max_val\n",
    "    return tensor\n",
    "\n",
    "def tensor2img(tensor):\n",
    "    tensor = tensor.cpu()\n",
    "    tensor = tensor.detach().numpy()\n",
    "    tensor = np.squeeze(tensor)\n",
    "    tensor = np.moveaxis(tensor, 0, 2)\n",
    "    tensor = (tensor * 255)  # + 0.5  # ? add 0.5 to rounding\n",
    "    tensor = tensor.clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    img = Image.fromarray(tensor)\n",
    "    return img\n",
    "\n",
    "def mkdir(directory, mode=0o777):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        os.chmod(directory, mode=mode)\n",
    "\n",
    "def dir_exists(directory):\n",
    "    return os.path.exists(directory)\n",
    "\n",
    "def crop(img_arr, block_size):\n",
    "    h_b, w_b = block_size\n",
    "    v_splited = np.vsplit(img_arr, img_arr.shape[0]//h_b)\n",
    "    h_splited = np.concatenate(\n",
    "        [np.hsplit(col, img_arr.shape[1]//w_b) for col in v_splited], 0)\n",
    "    return h_splited\n",
    "\n",
    "def generate_patches(src_path, files, set_path, crop_size, img_format, max_patches):\n",
    "    img_path = os.path.join(src_path, files)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    name, _ = files.split('.')\n",
    "    filedir = os.path.join(set_path, 'a')\n",
    "    if not dir_exists(filedir):\n",
    "        mkdir(filedir)\n",
    "        \n",
    "    filedirb = os.path.join(set_path, 'b')\n",
    "    if not dir_exists(filedirb):\n",
    "        mkdir(filedirb)\n",
    "\n",
    "    img = np.array(img)\n",
    "    h, w = img.shape[0], img.shape[1]\n",
    "\n",
    "    if crop_size == None:\n",
    "        img = np.copy(img)\n",
    "        img_patches = np.expand_dims(img, 0)\n",
    "    else:\n",
    "        rem_h = (h % crop_size[0])\n",
    "        rem_w = (w % crop_size[1])\n",
    "        img = img[:h-rem_h, :w-rem_w]\n",
    "        img_patches = crop(img, crop_size)\n",
    "\n",
    "    # print('Cropped')\n",
    "\n",
    "    n = 0\n",
    "\n",
    "    for i in range(min(len(img_patches), max_patches)):\n",
    "        img = Image.fromarray(img_patches[i])\n",
    "        img1 = Image.fromarray(img_patches[i])\n",
    "        img1 = transforms.ToTensor()(img1)\n",
    "        img2 = transforms.ToTensor()(img)\n",
    "        img3 = _compress(img1, 5)\n",
    "        img_compressed = tensor2img(img3)\n",
    "        \n",
    "        \n",
    "        img_compressed.save(\n",
    "            os.path.join(filedirb, '{}_{}.{}'.format(name, i, img_format))\n",
    "        )\n",
    "        img.save(\n",
    "            os.path.join(filedir, '{}_{}.{}'.format(name, i, img_format))\n",
    "        )\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    return n\n",
    "\n",
    "\n",
    "def main(target_dataset_folder, dataset_path, crop_size, img_format, max_patches, max_n):\n",
    "    print('[ Creating Dataset ]')\n",
    "    print('Crop Size : {}'.format(crop_size))\n",
    "    print('Target       : {}'.format(target_dataset_folder))\n",
    "    print('Dataset       : {}'.format(dataset_path))\n",
    "    print('Format    : {}'.format(img_format))\n",
    "    print('Max N    : {}'.format(max_n))\n",
    "\n",
    "    src_path = dataset_path\n",
    "    if not dir_exists(src_path):\n",
    "        raise(RuntimeError('Source folder not found, please put your dataset there'))\n",
    "\n",
    "    set_path = target_dataset_folder\n",
    "\n",
    "    mkdir(set_path)\n",
    "\n",
    "    img_files = os.listdir(src_path)\n",
    "\n",
    "    max = len(img_files)\n",
    "    bar = tqdm(img_files)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for files in bar:\n",
    "        k = generate_patches(src_path, files, set_path,\n",
    "                             crop_size, img_format, max_patches)\n",
    "\n",
    "        bar.set_description(desc='itr: %d/%d' % (\n",
    "            i, max\n",
    "        ))\n",
    "\n",
    "        j += k\n",
    "\n",
    "        if j >= max_n:\n",
    "            # Stop the process\n",
    "            print('Dataset count has been fullfuled')\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print('Dataset Created')\n",
    "\n",
    "main('dataset/train', '/home/sd/zsq_python/DIV2K_train_HR/', [128, 128], 'PNG', 10, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Creating Dataset ]\n",
      "Crop Size : [128, 128]\n",
      "Target       : dataset/validation\n",
      "Dataset       : /home/sd/zsq_python/DIV2K_valid_HR/\n",
      "Format    : PNG\n",
      "Max N    : 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "itr: 49/100:  49%|████▉     | 49/100 [00:13<00:14,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset count has been fullfuled\n",
      "Dataset Created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main('dataset/validation', '/home/sd/zsq_python/DIV2K_valid_HR/', [128, 128], 'PNG', 1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "def show_tensor(tensor):\n",
    "    plt.imshow(tensor.permute(1, 2, 0))\n",
    "\n",
    "class ModelSubtraction(nn.Module):\n",
    "    def __init__(self, n_layers):\n",
    "        super(ModelSubtraction, self).__init__()\n",
    "        \n",
    "        networks = [\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            networks += [\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "        \n",
    "        networks += [\n",
    "            nn.Conv2d(64, 3, kernel_size=5, stride=1, padding=2),\n",
    "                nn.ReLU()\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*networks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return x - y\n",
    "\n",
    "class ModelAddition(nn.Module):\n",
    "    def __init__(self, n_layers):\n",
    "        super(ModelAddition, self).__init__()\n",
    "        \n",
    "        networks = [\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            networks += [\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "        \n",
    "        networks += [\n",
    "            nn.Conv2d(64, 3, kernel_size=5, stride=1, padding=2),\n",
    "                nn.ReLU()\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*networks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y + x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_layers_sub = 6, n_layers_add = 6):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.subs = ModelSubtraction(n_layers=n_layers_sub)\n",
    "        self.add = ModelAddition(n_layers=n_layers_add)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.subs(x)\n",
    "        y = self.add(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".PNG\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.a_path = os.path.join(image_dir, \"a\")\n",
    "        self.b_path = os.path.join(image_dir, \"b\")\n",
    "\n",
    "        self.image_filenames = [x for x in listdir(\n",
    "            self.a_path) if is_image_file(x)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        a = Image.open(\n",
    "            os.path.join(self.a_path, self.image_filenames[index])).convert('RGB')\n",
    "        \n",
    "        b = Image.open(\n",
    "            os.path.join(self.b_path, self.image_filenames[index])).convert('RGB')\n",
    "        \n",
    "        a = transforms.ToTensor()(a)\n",
    "        b = transforms.ToTensor()(b)\n",
    "        \n",
    "        return a, b\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "itr: 2000/2000 [  0/100] Loss: 0.000088: 100%|██████████| 2000/2000 [03:00<00:00, 11.05it/s]\n",
      "itr: 2000/2000 [  1/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:16<00:00, 10.18it/s]\n",
      "itr: 2000/2000 [  2/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:16<00:00, 10.18it/s]\n",
      "itr: 2000/2000 [  3/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:15<00:00, 10.24it/s]\n",
      "itr: 2000/2000 [  4/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:12<00:00, 10.39it/s]\n",
      "itr: 2000/2000 [  5/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.53it/s]\n",
      "itr: 2000/2000 [  6/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.54it/s]\n",
      "itr: 2000/2000 [  7/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.55it/s]\n",
      "itr: 2000/2000 [  8/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [  9/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 10/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 11/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 12/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:10<00:00, 10.52it/s]\n",
      "itr: 2000/2000 [ 13/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 14/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 15/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 16/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.56it/s]\n",
      "itr: 2000/2000 [ 17/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 19/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 20/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 21/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 22/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.56it/s]\n",
      "itr: 2000/2000 [ 23/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.56it/s]\n",
      "itr: 2000/2000 [ 24/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 25/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 26/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 27/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 28/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 29/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 30/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 31/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.59it/s]\n",
      "itr: 2000/2000 [ 32/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.59it/s]\n",
      "itr: 2000/2000 [ 33/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 34/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 35/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 36/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 37/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 38/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 39/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.59it/s]\n",
      "itr: 2000/2000 [ 40/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.59it/s]\n",
      "itr: 2000/2000 [ 41/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 42/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 43/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 44/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 45/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 46/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.59it/s]\n",
      "itr: 2000/2000 [ 47/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.60it/s]\n",
      "itr: 2000/2000 [ 48/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 49/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.59it/s]\n",
      "itr: 2000/2000 [ 50/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.59it/s]\n",
      "itr: 2000/2000 [ 51/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 52/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 53/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 54/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.59it/s]\n",
      "itr: 2000/2000 [ 55/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.60it/s]\n",
      "itr: 2000/2000 [ 56/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.60it/s]\n",
      "itr: 2000/2000 [ 57/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 58/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.59it/s]\n",
      "itr: 2000/2000 [ 59/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.60it/s]\n",
      "itr: 2000/2000 [ 60/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 61/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 62/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 63/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 64/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 65/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.57it/s]\n",
      "itr: 2000/2000 [ 66/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:09<00:00, 10.58it/s]\n",
      "itr: 2000/2000 [ 67/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.60it/s]\n",
      "itr: 2000/2000 [ 68/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.61it/s]\n",
      "itr: 2000/2000 [ 69/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:10<00:00, 10.48it/s]\n",
      "itr: 2000/2000 [ 70/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.61it/s]\n",
      "itr: 2000/2000 [ 71/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.61it/s]\n",
      "itr: 2000/2000 [ 72/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.62it/s]\n",
      "itr: 2000/2000 [ 73/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.64it/s]\n",
      "itr: 2000/2000 [ 74/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.63it/s]\n",
      "itr: 2000/2000 [ 75/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.64it/s]\n",
      "itr: 2000/2000 [ 76/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.65it/s]\n",
      "itr: 2000/2000 [ 77/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.63it/s]\n",
      "itr: 2000/2000 [ 78/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.64it/s]\n",
      "itr: 2000/2000 [ 79/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:08<00:00, 10.62it/s]\n",
      "itr: 2000/2000 [ 80/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.64it/s]\n",
      "itr: 2000/2000 [ 81/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.66it/s]\n",
      "itr: 2000/2000 [ 82/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.65it/s]\n",
      "itr: 2000/2000 [ 83/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.67it/s]\n",
      "itr: 2000/2000 [ 84/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.67it/s]\n",
      "itr: 2000/2000 [ 85/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.68it/s]\n",
      "itr: 2000/2000 [ 86/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.68it/s]\n",
      "itr: 2000/2000 [ 87/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.67it/s]\n",
      "itr: 2000/2000 [ 88/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.67it/s]\n",
      "itr: 2000/2000 [ 89/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.68it/s]\n",
      "itr: 2000/2000 [ 90/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.69it/s]\n",
      "itr: 2000/2000 [ 91/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.69it/s]\n",
      "itr: 2000/2000 [ 92/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.69it/s]\n",
      "itr: 2000/2000 [ 93/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.66it/s]\n",
      "itr: 2000/2000 [ 94/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.68it/s]\n",
      "itr: 2000/2000 [ 95/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.68it/s]\n",
      "itr: 2000/2000 [ 96/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.67it/s]\n",
      "itr: 2000/2000 [ 97/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.66it/s]\n",
      "itr: 2000/2000 [ 98/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.67it/s]\n",
      "itr: 2000/2000 [ 99/100] Loss: 0.000087: 100%|██████████| 2000/2000 [03:07<00:00, 10.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.83823483630491, 40.62857213441958, 40.6447251965017, 40.65049609190179, 40.61640899277512, 40.765732168951054, 40.93943347407066, 40.89945428499812, 40.896184851667236, 40.821677001026394, 40.838087108170754, 40.897864664176495, 40.89354902476665, 40.98688635541137, 40.950558830053794, 41.007984460122444, 40.97508882106988, 41.02238014549009, 40.99916813764918, 40.96081649348479, 40.950276437106076, 40.93904622971211, 40.91355717622274, 40.948311138036075, 40.98317397615884, 41.02907523440226, 41.068056571818985, 41.05275580288886, 41.02950430828962, 41.00768581917668, 41.01068412999176, 40.98802693105935, 40.981896239459644, 40.9650619953726, 40.96028797991337, 40.99033131316943, 40.97765487222852, 40.968152155449, 40.98565685199935, 40.96843726854176, 40.958072419139484, 40.940363850976, 40.928746765753544, 40.91876469231091, 40.94160485717705, 40.97170940574933, 40.96516905244159, 40.959771483770666, 40.95645016545658, 40.977786197227836, 40.96515826798632, 40.955185158029934, 40.95560630189378, 40.94736079217014, 40.96300430295442, 40.95651306955784, 40.94416097470524, 40.95990170693716, 40.95833671291995, 40.951702364445254, 40.96784042608662, 40.98087540001973, 40.974431958254534, 40.972758611299106, 40.969661638826814, 40.958012202695144, 40.97029320936317, 40.98602606176372, 40.973374981571766, 40.96910838075073, 40.962066227842655, 40.95091833853886, 40.93961612077903, 40.936070225724876, 40.94793715875738, 40.94469516457304, 40.933201864151265, 40.92462396263483, 40.91781431517722, 40.92787890190955, 40.92143110287998, 40.91857971773245, 40.93013310101056, 40.94242840548149, 40.93420811826256, 40.93393075699165, 40.943478718941854, 40.941304866718774, 40.937130002646285, 40.93136632013886, 40.92768759506278, 40.92698678449006, 40.93626335121075, 40.92860023144361, 40.92078326461859, 40.91385470299082, 40.906015982780055, 40.917183840686604, 40.912972340313544, 40.90422249411058]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "import statistics\n",
    "from torchinfo import summary\n",
    "\n",
    "def psnr(ground, compressed):\n",
    "    np_ground = np.array(ground, dtype='float')\n",
    "    np_compressed = np.array(compressed, dtype='float')\n",
    "    mse = np.mean((np_ground - np_compressed)**2)\n",
    "    psnr = np.log10(255**2/mse) * 10\n",
    "    return psnr\n",
    "\n",
    "torch.cuda.manual_seed(100)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_set = DatasetFromFolder('dataset/train')\n",
    "training_data_loader = DataLoader(\n",
    "        dataset=train_set, num_workers=4, batch_size=4, shuffle=True)\n",
    "validate_set = DatasetFromFolder('dataset/validation')\n",
    "validate_data_loader = DataLoader(\n",
    "        dataset=validate_set, num_workers=4, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "#image = plt.imread('dataset/train/b/0802x2_3.PNG')\n",
    "\n",
    "#image_tensor = transforms.ToTensor()(image) # (C, H, W) -> (N, C, H, W)\n",
    "#image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "net = Model().to(device)\n",
    "\n",
    "summ = summary(net, input_size=(4, 3, 128, 128))\n",
    "\n",
    "optimizer = optim.Adam(net.parameters() ,lr=0.001)\n",
    "\n",
    "epoch = 100\n",
    "\n",
    "list_psnr = list()\n",
    "psnr_sum = list()\n",
    "for i in range(epoch):\n",
    "    data_len = len(training_data_loader)\n",
    "    datas = tqdm(enumerate(training_data_loader, 1), total=data_len)\n",
    "    loss_o_sum = 0\n",
    "    \n",
    "    data_len_valid = len(validate_data_loader)\n",
    "    datas_valid = tqdm(enumerate(validate_data_loader, 1), total=data_len_valid, disable=True)\n",
    "    \n",
    "    for iteration, batch in datas:\n",
    "        image_a, image_b = batch[0].to(device), batch[1].to(device)\n",
    "        #image = torch.randn((1, 3, 128, 128))\n",
    "        optimizer.zero_grad() #Membuat gradien model menjadi 0\n",
    "        output = net(image_b) #forward input ke model\n",
    "        loss_o = loss(output, image_a) #hitung loss\n",
    "        loss_o.backward() #menghitung gradien dari model\n",
    "        \n",
    "        optimizer.step() #mengoptimasi parameter model\n",
    "        loss_o_sum += loss_o.item()\n",
    "\n",
    "        datas.set_description(desc='itr: %d/%d [%3d/%3d] Loss: %.6f' % (\n",
    "                    iteration, data_len, i, epoch, loss_o_sum/max(1, iteration)\n",
    "                ))\n",
    "    for iteration, batch in datas_valid:\n",
    "        valid_a, valid_b = batch[0].to(device), batch[1].to(device)\n",
    "        outimg = net(valid_b)\n",
    "        hasil = tensor2img(outimg[0])\n",
    "        source = tensor2img(valid_a[0])\n",
    "        psnr_o = psnr(hasil, source)\n",
    "        psnr_sum.append(psnr_o)\n",
    "        \n",
    "    list_psnr.append(statistics.mean(psnr_sum))\n",
    "        \n",
    "torch.save(net, 'model.pth')\n",
    "print(list_psnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: model.pth (deflated 8%)\n",
      "  adding: psnr.csv (deflated 55%)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "!rm -rf dataset\n",
    "\n",
    "d = {'PSNR': list_psnr}\n",
    "df = pandas.DataFrame(data=d)\n",
    "df.to_csv('psnr.csv', index=False)\n",
    "\n",
    "!zip hasil_training_DRCNN_5bit.zip model.pth psnr.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    --                        --\n",
       "├─ModelSubtraction: 1-1                  [4, 3, 128, 128]          --\n",
       "│    └─Sequential: 2-1                   [4, 3, 128, 128]          --\n",
       "│    │    └─Conv2d: 3-1                  [4, 64, 128, 128]         4,864\n",
       "│    │    └─ReLU: 3-2                    [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-3                  [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-4                    [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-5                  [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-6                    [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-7                  [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-8                    [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-9                  [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-10                   [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-11                 [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-12                   [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-13                 [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-14                   [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-15                 [4, 3, 128, 128]          4,803\n",
       "│    │    └─ReLU: 3-16                   [4, 3, 128, 128]          --\n",
       "├─ModelAddition: 1-2                     [4, 3, 128, 128]          --\n",
       "│    └─Sequential: 2-2                   [4, 3, 128, 128]          --\n",
       "│    │    └─Conv2d: 3-17                 [4, 64, 128, 128]         4,864\n",
       "│    │    └─ReLU: 3-18                   [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-19                 [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-20                   [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-21                 [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-22                   [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-23                 [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-24                   [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-25                 [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-26                   [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-27                 [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-28                   [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-29                 [4, 64, 128, 128]         36,928\n",
       "│    │    └─ReLU: 3-30                   [4, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-31                 [4, 3, 128, 128]          4,803\n",
       "│    │    └─ReLU: 3-32                   [4, 3, 128, 128]          --\n",
       "==========================================================================================\n",
       "Total params: 462,470\n",
       "Trainable params: 462,470\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 30.31\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 472.91\n",
       "Params size (MB): 1.85\n",
       "Estimated Total Size (MB): 475.54\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
